#!/usr/bin/env python3
"""
Script ƒë·ªÉ ƒë√°nh gi√° t·∫•t c·∫£ 60 model ch∆∞a qua training tr√™n datasets SumMe v√† TVSum
M·ª•c ƒë√≠ch: So s√°nh hi·ªáu nƒÉng random initialization vs trained models
"""

import os
import os.path as osp
import argparse
import h5py
import numpy as np
import torch
import json
from tabulate import tabulate
import time
from datetime import datetime

from models import DSN
from utils import read_json
import vsum_tools

# C·∫•u h√¨nh datasets
DATASETS = {
    'summe': {
        'h5_path': 'datasets/eccv16_dataset_summe_google_pool5.h5',
        'split_path': 'datasets/summe_splits.json',
        'metric': 'summe'
    },
    'tvsum': {
        'h5_path': 'datasets/eccv16_dataset_tvsum_google_pool5.h5', 
        'split_path': 'datasets/tvsum_splits.json',
        'metric': 'tvsum'
    }
}

# C·∫•u h√¨nh model architectures
MODEL_CONFIGS = {
    'DR-DSN': {'reward_type': 'dr', 'supervised': False, 'sup_only': False},
    'D-DSN': {'reward_type': 'd', 'supervised': False, 'sup_only': False},
    'D-DSN-nolambda': {'reward_type': 'd-nolambda', 'supervised': False, 'sup_only': False},
    'R-DSN': {'reward_type': 'r', 'supervised': False, 'sup_only': False},
    'DR-DSNsup': {'reward_type': 'dr', 'supervised': True, 'sup_only': False},
    'DSNsup': {'reward_type': 'dr', 'supervised': True, 'sup_only': True}
}

def create_untrained_model(model_type, input_dim=1024, hidden_dim=256, num_layers=1, rnn_cell='lstm'):
    """
    T·∫°o model ch∆∞a qua training v·ªõi random weights.
    ƒê·∫£m b·∫£o c·∫•u h√¨nh ƒë√∫ng v·ªõi t·ª´ng lo·∫°i model (DR-DSN, DR-DSNsup, DSNsup, v.v.)
    
    Kh√°c bi·ªát gi·ªØa c√°c model:
    - DR-DSN: S·ª≠ d·ª•ng c·∫£ diversity v√† representativeness trong reward
    - D-DSN: Ch·ªâ s·ª≠ d·ª•ng diversity trong reward
    - R-DSN: Ch·ªâ s·ª≠ d·ª•ng representativeness trong reward  
    - D-DSN-nolambda: Diversity nh∆∞ng kh√¥ng c√≥ lambda regularization
    - DR-DSNsup: DR-DSN c√≥ th√™m supervised learning (k·∫øt h·ª£p RL v√† SL)
    - DSNsup: Ch·ªâ s·ª≠ d·ª•ng supervised learning, kh√¥ng c√≥ RL
    
    Tham s·ªë:
    - model_type: Lo·∫°i m√¥ h√¨nh (DR-DSN, D-DSN, R-DSN, DR-DSNsup, ...)
    """
    config = MODEL_CONFIGS[model_type]
    
    # 1. T·∫°o model v·ªõi random initialization
    model = DSN(in_dim=input_dim, hid_dim=hidden_dim, num_layers=num_layers, cell=rnn_cell)
    
    # 2. Thi·∫øt l·∫≠p c√°c thu·ªôc t√≠nh ph√π h·ª£p v·ªõi lo·∫°i model
    model.reward_type = config['reward_type']
    model.supervised = config['supervised']
    model.sup_only = config['sup_only']
    model.model_type = model_type  # L∆∞u th√™m model type
    
    # 3. Log th√¥ng tin ki·∫øn tr√∫c ƒë·ªÉ debug
    model_str = (f"Model: {model_type} ("
                f"{'Supervised' if config['supervised'] else 'Unsupervised'}, "
                f"Reward: {config['reward_type']}"
                f"{', Supervised-only' if config['sup_only'] else ''})")
    print(f"  - T·∫°o untrained model: {model_str}")
    
    # T·∫°o m·ªôt model description chi ti·∫øt
    if model_type == 'DR-DSN':
        model.description = "Diversity + Representativeness DSN (RL)"
    elif model_type == 'D-DSN':
        model.description = "Diversity-only DSN (RL)"
    elif model_type == 'R-DSN':
        model.description = "Representativeness-only DSN (RL)" 
    elif model_type == 'D-DSN-nolambda':
        model.description = "Diversity DSN without lambda (RL)"
    elif model_type == 'DR-DSNsup':
        model.description = "Diversity + Representativeness + Supervised DSN (RL+SL)"
    elif model_type == 'DSNsup':
        model.description = "Supervised-only DSN (SL)"
    
    # Kh√¥ng load weights, gi·ªØ nguy√™n random initialization
    return model

def evaluate_model(model, dataset, test_keys, metric_type, verbose=False):
    """ƒê√°nh gi√° model tr√™n test set"""
    model.eval()
    eval_metric = 'avg' if metric_type == 'tvsum' else 'max'
    
    fms = []
    eval_arr = []
    
    with torch.no_grad():
        for key_idx, key in enumerate(test_keys):
            seq = dataset[key]['features'][...]
            seq = torch.from_numpy(seq).unsqueeze(0)
            
            if torch.cuda.is_available():
                seq = seq.cuda()
            
            # Forward pass ƒë·ªÉ l·∫•y scores
            probs = model(seq)
            probs = probs.data.cpu().squeeze().numpy()
            
            cps = dataset[key]['change_points'][...]
            num_frames = dataset[key]['n_frames'][()]
            nfps = dataset[key]['n_frame_per_seg'][...].tolist()
            positions = dataset[key]['picks'][...]
            user_summary = dataset[key]['user_summary'][...]
            
            # T·∫°o machine summary t·ª´ probability scores
            machine_summary = vsum_tools.generate_summary(probs, cps, num_frames, nfps, positions)
            
            # T√≠nh F-measure
            fm, _, _ = vsum_tools.evaluate_summary(machine_summary, user_summary, eval_metric)
            fms.append(fm)
            
            eval_arr.append([key_idx, key, "{:.1%}".format(fm)])
            
            if verbose:
                print(f"  Video {key_idx+1}/{len(test_keys)}: {key} F1={fm:.1%}")
    
    mean_fm = np.mean(fms)
    return mean_fm, eval_arr

def evaluate_all_untrained_models(output_file='untrained_model_evaluation.json', verbose=False, seed=42):
    """ƒê√°nh gi√° t·∫•t c·∫£ 60 model combinations ch∆∞a qua training"""
    
    print("\n" + "=" * 80)
    print(" " * 20 + "ƒê√ÅNH GI√Å 60 MODEL CH∆ØA QUA TRAINING")
    print("=" * 80)
    print(f"üïí B·∫Øt ƒë·∫ßu l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Kh·ªüi t·∫°o th·ªùi gian b·∫Øt ƒë·∫ßu ƒë·ªÉ t√≠nh to√°n ti·∫øn ƒë·ªô
    start_time_global = time.time()
    
    # Thi·∫øt l·∫≠p seed cho reproducibility n·∫øu c·∫ßn
    if seed is not None:
        print(f"üî¢ S·ª≠ d·ª•ng random seed: {seed}")
        torch.manual_seed(seed)
        np.random.seed(seed)
        import random
        random.seed(seed)
    
    results = {}
    summary_table = []
    
    # Thi·∫øt l·∫≠p GPU n·∫øu c√≥
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        device_name = torch.cuda.get_device_name(0)
        memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB
        print(f"üñ•Ô∏è  S·ª≠ d·ª•ng GPU: {device_name} ({memory:.1f} GB)")
    else:
        print("üíª S·ª≠ d·ª•ng CPU ƒë·ªÉ ƒë√°nh gi√°")
    print()
    
    total_evaluations = len(MODEL_CONFIGS) * len(DATASETS) * 5  # 6 models √ó 2 datasets √ó 5 splits = 60
    current_eval = 0
    
    for model_type in MODEL_CONFIGS.keys():
        results[model_type] = {}
        
        for dataset_name in DATASETS.keys():
            dataset_config = DATASETS[dataset_name]
            results[model_type][dataset_name] = {}
            
            # Load dataset
            print(f"Loading dataset: {dataset_config['h5_path']}")
            dataset = h5py.File(dataset_config['h5_path'], 'r')
            splits = read_json(dataset_config['split_path'])
            
            split_scores = []
            
            for split_id in range(5):  # 5-fold cross validation
                current_eval += 1
                progress = (current_eval / total_evaluations) * 100
                elapsed_time = time.time() - start_time_global if 'start_time_global' in locals() else 0
                
                # ∆Ø·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i
                time_per_eval = elapsed_time / current_eval if current_eval > 0 else 0
                remaining_evals = total_evaluations - current_eval
                est_remaining = time_per_eval * remaining_evals if current_eval > 0 else "ƒêang t√≠nh..."
                
                # ƒê·ªãnh d·∫°ng th·ªùi gian c√≤n l·∫°i
                if isinstance(est_remaining, str):
                    time_str = est_remaining
                else:
                    mins_remaining = int(est_remaining // 60)
                    secs_remaining = int(est_remaining % 60)
                    time_str = f"{mins_remaining}m {secs_remaining}s"
                
                print(f"[{progress:5.1f}%] [{current_eval:2d}/{total_evaluations}] "
                      f"ƒê√°nh gi√° {model_type:>10} tr√™n {dataset_name.upper():>6} Split {split_id} "
                      f"(C√≤n l·∫°i: {time_str})")
                
                # T·∫°o model ch∆∞a qua training v·ªõi ki·∫øn tr√∫c ph√π h·ª£p
                model = create_untrained_model(model_type)
                
                # X√°c th·ª±c lo·∫°i model ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng
                config = MODEL_CONFIGS[model_type]
                assert hasattr(model, 'reward_type'), f"Model kh√¥ng c√≥ thu·ªôc t√≠nh reward_type"
                assert model.reward_type == config['reward_type'], f"reward_type kh√¥ng kh·ªõp: {model.reward_type} vs {config['reward_type']}"
                
                if use_gpu:
                    model = model.cuda()
                
                # L·∫•y test keys cho split n√†y
                split = splits[split_id]
                test_keys = split['test_keys']
                
                # ƒê√°nh gi√° model
                start_time = time.time()
                mean_fm, eval_details = evaluate_model(
                    model, dataset, test_keys, 
                    dataset_config['metric'], verbose=verbose
                )
                eval_time = time.time() - start_time
                
                split_scores.append(mean_fm)
                results[model_type][dataset_name][f'split_{split_id}'] = {
                    'f1_score': mean_fm,
                    'eval_time': eval_time,
                    'details': eval_details
                }
                
                print(f"  ‚úì F1-Score: {mean_fm:.1%} (th·ªùi gian: {eval_time:.1f}s)")
                
                # Cleanup GPU memory
                del model
                if use_gpu:
                    torch.cuda.empty_cache()
            
            # T√≠nh trung b√¨nh cho dataset n√†y
            mean_score = np.mean(split_scores)
            std_score = np.std(split_scores)
            
            results[model_type][dataset_name]['average'] = {
                'mean': mean_score,
                'std': std_score,
                'splits': split_scores
            }
            
            # Th√™m v√†o b·∫£ng t·ªïng k·∫øt
            splits_str = ", ".join([f"S{i}:{score:.1%}" for i, score in enumerate(split_scores)])
            summary_table.append([
                model_type,
                dataset_name.upper(),
                f"{mean_score:.1%} ¬± {std_score:.1%}",
                f"[{splits_str}]"
            ])
            
            dataset.close()
            print()
    
    # In k·∫øt qu·∫£ t·ªïng k·∫øt
    print("=" * 120)
    print("B·∫¢NG T·ªîNG K·∫æT K·∫æT QU·∫¢ ƒê√ÅNH GI√Å MODEL CH∆ØA QUA TRAINING")
    print("=" * 120)
    
    headers = ["Model Type", "Dataset", "Average F1 ¬± Std", "Split Details"]
    print(tabulate(summary_table, headers=headers, tablefmt="grid"))
    
    # So s√°nh v·ªõi trained models (n·∫øu c√≥)
    print("\n" + "=" * 80)
    print("SO S√ÅNH V·ªöI TRAINED MODELS")
    print("=" * 80)
    
    trained_scores = {
        'DR-DSN': {'summe': 0.399, 'tvsum': 0.564},
        'D-DSN': {'summe': 0.405, 'tvsum': 0.557},
        'R-DSN': {'summe': 0.388, 'tvsum': 0.566},
        'DR-DSNsup': {'summe': 0.419, 'tvsum': 0.567},
        'DSNsup': {'summe': 0.392, 'tvsum': 0.523}
    }
    
    comparison_table = []
    for model_type in MODEL_CONFIGS.keys():
        for dataset_name in DATASETS.keys():
            if model_type in trained_scores and dataset_name in trained_scores[model_type]:
                untrained_score = results[model_type][dataset_name]['average']['mean']
                trained_score = trained_scores[model_type][dataset_name]
                improvement = trained_score - untrained_score
                improvement_pct = (improvement / untrained_score) * 100 if untrained_score > 0 else 0
                
                comparison_table.append([
                    model_type,
                    dataset_name.upper(),
                    f"{untrained_score:.1%}",
                    f"{trained_score:.1%}",
                    f"+{improvement:.1%}",
                    f"+{improvement_pct:.1f}%"
                ])
    
    comp_headers = ["Model", "Dataset", "Untrained F1", "Trained F1", "Improvement", "Relative %"]
    print(tabulate(comparison_table, headers=comp_headers, tablefmt="grid"))
    
    # L∆∞u k·∫øt qu·∫£
    results['evaluation_info'] = {
        'timestamp': datetime.now().isoformat(),
        'total_models_evaluated': total_evaluations,
        'gpu_used': use_gpu,
        'random_seed': seed,
        'model_configs': {k: {'reward': v['reward_type'], 
                             'supervised': v['supervised'], 
                             'sup_only': v['sup_only']} for k, v in MODEL_CONFIGS.items()},
        'summary': summary_table,
        'comparison': comparison_table
    }
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nK·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_file}")
    print(f"Ho√†n th√†nh l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="ƒê√°nh gi√° t·∫•t c·∫£ model ch∆∞a qua training")
    parser.add_argument('--output', type=str, default='untrained_model_evaluation.json',
                       help="File output ƒë·ªÉ l∆∞u k·∫øt qu·∫£ (default: untrained_model_evaluation.json)")
    parser.add_argument('--verbose', action='store_true',
                       help="Hi·ªÉn th·ªã chi ti·∫øt t·ª´ng video")
    parser.add_argument('--seed', type=int, default=42,
                      help="Random seed ƒë·ªÉ kh·ªüi t·∫°o model reproducible (default: 42)")
    parser.add_argument('--no-seed', action='store_true',
                      help="Kh√¥ng thi·∫øt l·∫≠p random seed (k·∫øt qu·∫£ s·∫Ω kh√°c m·ªói l·∫ßn)")
    parser.add_argument('--no-gpu', action='store_true',
                      help="Kh√¥ng s·ª≠ d·ª•ng GPU ngay c·∫£ khi kh·∫£ d·ª•ng")
    
    args = parser.parse_args()
    
    # Ki·ªÉm tra datasets c√≥ t·ªìn t·∫°i kh√¥ng
    for dataset_name, config in DATASETS.items():
        if not osp.exists(config['h5_path']):
            print(f"ERROR: Dataset {config['h5_path']} kh√¥ng t·ªìn t·∫°i!")
            return
        if not osp.exists(config['split_path']):
            print(f"ERROR: Split file {config['split_path']} kh√¥ng t·ªìn t·∫°i!")
            return
    
    # V√¥ hi·ªáu h√≥a GPU n·∫øu y√™u c·∫ßu
    if args.no_gpu:
        print("C·ªù --no-gpu ƒë∆∞·ª£c ƒë·∫∑t: Ch·∫°y tr√™n CPU thay v√¨ GPU")
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # V√¥ hi·ªáu h√≥a CUDA
    
    # Ch·∫°y ƒë√°nh gi√°
    seed = None if args.no_seed else args.seed
    results = evaluate_all_untrained_models(args.output, args.verbose, seed=seed)

if __name__ == '__main__':
    main()
